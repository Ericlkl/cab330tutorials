{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3: Decision Tree Mining\n",
    "\n",
    "### What's on this week\n",
    "1. [Resuming from week 2](#resume)\n",
    "2. [Building your first decision tree model](#build)\n",
    "3. [Understanding and visualising your decision tree](#viz)\n",
    "4. [Finding optimal hyperparameters with GridSearchCV](#gridsearch)\n",
    "\n",
    "---\n",
    "\n",
    "### Important Changelog:\n",
    "* (25/07/2017) Made tutorial notes public.\n",
    "* (09/08/2017) Added explanation on classification report metrics and DT visualisation.\n",
    "* (14/08/2017) Updated explanation for some parts.\n",
    "\n",
    "The practical note for this week introduces you to build decision tree predictive models in Python. Decision trees are relatively simple, yet it can be powerful if built and utilised properly. These models will help us to classify the lapsing donors based on their responses to greeting card mailing conducted by the national veteran organisation.\n",
    "\n",
    "Predictive modelling, including decision trees, start with a training data set. Observations in a training data set are known as *training cases/samples/rows/instances/records*. The variables are called *inputs* (or *variables/attributes/columns/features/explanatory variables or independent variables*) and *targets* (or *response/outcome/output/class/dependent variables*). For a given case, the inputs are used to estimate target.\n",
    "\n",
    "As you learned in week 2, measurement type/role for inputs and target can be varied. In this case, we are building predictive models to predict TARGETB, which is a binary variable.\n",
    "\n",
    "**This tutorial notes is in experimental version. Please give us feedbacks and suggestions on how to make it better. Ask your tutor for any question and clarification.**\n",
    "\n",
    "## 1. Resuming from week 2 <a name=\"resume\"></a>\n",
    "Last week, we learned how to perform data preparation on our dataset. We build plots of data distribution, dealt with noisy values, imputed missing values and drop columns not required in our analysis. After week 2, your code should look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# inside dm_tools.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def data_prep():\n",
    "    # read the pva97nk dataset\n",
    "    df = pd.read_csv('pva97nk.csv')\n",
    "    \n",
    "    # change DemCluster from interval/integer to nominal/str\n",
    "    df['DemCluster'] = df['DemCluster'].astype(str)\n",
    "    \n",
    "    # change DemHomeOwner into binary 0/1 variable\n",
    "    dem_home_owner_map = {'U':0, 'H': 1}\n",
    "    df['DemHomeOwner'] = df['DemHomeOwner'].map(dem_home_owner_map)\n",
    "    \n",
    "    # denote errorneous values in DemMidIncome\n",
    "    mask = df['DemMedIncome'] < 1\n",
    "    df.loc[mask, 'DemMedIncome'] = np.nan\n",
    "    \n",
    "    # impute missing values in DemAge with its mean\n",
    "    df['DemAge'].fillna(df['DemAge'].mean(), inplace=True)\n",
    "\n",
    "    # impute med income using mean\n",
    "    df['DemMedIncome'].fillna(df['DemMedIncome'].mean(), inplace=True)\n",
    "\n",
    "    # impute gift avg card 36 using mean\n",
    "    df['GiftAvgCard36'].fillna(df['GiftAvgCard36'].mean(), inplace=True)\n",
    "    \n",
    "    # drop ID and the unused target variable\n",
    "    df.drop(['ID', 'TargetD'], axis=1, inplace=True)\n",
    "    \n",
    "    # one-hot encoding\n",
    "    df = pd.get_dummies(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# in our project space\n",
    "from dm_tools import data_prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building your first decision tree <a name=\"build\"></a>\n",
    "\n",
    "Before building a decision tree, we need to set up partition for the data first. Given a set of training data, you can easily build a model that is highly accurate in that dataset itself. However, a model built under this circumstance typically \"overfits\" the dataset, generalises poorly and has horrible accuracy in predicting similar data outside of the training set.  We will see the example of overfitting soon.\n",
    "\n",
    "To avoid this pitfall, we need to set aside some data from our whole dataset to be used as \"test\". To split the data, use the `train_test_split` function from `sklearn.model_selection`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, let's partition our data. The convention in Python is to assign input variables as `X` and target as `y`. In our case, `y` would be `TargetB` and `X` would be the rest of the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# preprocessing step\n",
    "df = data_prep()\n",
    "\n",
    "# target/input split\n",
    "y = df['TargetB']\n",
    "X = df.drop(['TargetB'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you done that, convert `X` (which is still a pandas `DataFrame` object) into a `numpy` matrix that can be consumed by `sklearn`. Next, use the `train_test_split` function to split them into 50% training and 50% test data. Typically, this configuration is sufficient to train on. However, if the class distribution is uneven (e.g. if in our data set there are 70% non-donors to 30% repeat donors), we want to ensure there is enough representation of the minority class in the training set. In this case, we need larger training set, such as 70/30 or 66/33.\n",
    "\n",
    "In performing the split, we also need to stratify based on `y`. Stratification ensures the same ratio of positive and negative targets in both train and test data set.\n",
    "\n",
    "As `train_test_split` shuffles the dataset before splitting it, it is important to set a consistent random state, which is the seed number used to generate the shuffle. I am using 42 here, but you can use any integer number as long as it is consistent across this tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_mat = X.as_matrix()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_mat, y, test_size=0.5, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to build our model. Let's start by importing `DecisionTreeClassifier`, initialise a model, and then training it using `.fit` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# simple decision tree training\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have just trained your first model. Congratulations! :D\n",
    "\n",
    "Once the model is trained, it is important to assess its performance. A very common way to check the quality of a predictive classification model is `accuracy`. We can use `.score` to find out its accuracy against a specific set of data. Let's try it on train data first.\n",
    "\n",
    "\n",
    ">**Accuracy**: *Accuracy of a classifier model refers to the ability of the model to predict the labels of a dataset correctly. It is calculated by dividing the number of correct predictions by the number of data points.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Train accuracy:\", model.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is wonderful! It seems that the model has managed to learn all of the patterns in training data and is able to predict with 100% accuracy. However, this is not enough. We need to check whether it can replicate the performance on similar data that it is not trained on A.K.A. the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.517034895726\n"
     ]
    }
   ],
   "source": [
    "print(\"Test accuracy:\", model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whoops. it seems like the model actually performed much worse on the test. This is a clear indication of overfitting. We need to make our model generalize better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tips: Alternatively, we could also use `classification_report` to assess the model's prediction on test data. Classification report outputs a number of statistics for each target class:\n",
    "1. Precision: Proportion of all positive predictions that are correct. Precision is a measure of how many positive predictions were actual positive observations.\n",
    "2. Recall: Proportion of all real positive observations that are correct. Precision is a measure of how many actual positive observations were predicted correctly.\n",
    "3. F1: The harmonic mean of precision and recall. F1 score is an 'average' of both precision and recall.\n",
    "4. Support: Number of instances in each class.\n",
    "\n",
    "Read more: https://chrisalbon.com/machine-learning/precision_recall_and_F1_scores.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.52      0.53      0.52      2422\n",
      "          1       0.52      0.51      0.51      2421\n",
      "\n",
      "avg / total       0.52      0.52      0.52      4843\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Understanding and visualising your decision tree\n",
    "\n",
    "### 3.1. Feature importance\n",
    "\n",
    "Let's take a deeper look on the decision tree that we just built. Firstly, it is important to get insights of which features/input variables that are impactful to the decision making process in our model. This is commonly known as **feature importance**. In an `sklearn` decision tree, feature importance is stored within the model itself in form of **(feature_index, feature_importance_value)**. For easier interpretation, let's match this data with feature names from `X`, sort them in descending order of importance and print them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DemMedHomeValue : 0.0895170967109\n",
      "DemMedIncome : 0.0691420566238\n",
      "GiftAvgAll : 0.06185764539\n",
      "DemAge : 0.0589333869094\n",
      "DemPctVeterans : 0.055504987849\n",
      "GiftAvg36 : 0.0522582494486\n",
      "GiftTimeFirst : 0.0466620076413\n",
      "GiftTimeLast : 0.0441263227433\n",
      "GiftAvgLast : 0.0386727704565\n",
      "PromCntAll : 0.035839855936\n",
      "PromCnt36 : 0.0348942778523\n",
      "GiftAvgCard36 : 0.0335087972723\n",
      "PromCntCardAll : 0.0328380316539\n",
      "PromCntCard36 : 0.030855320989\n",
      "GiftCntCardAll : 0.0283423319115\n",
      "GiftCnt36 : 0.0280352274069\n",
      "PromCnt12 : 0.0278555955027\n",
      "GiftCntAll : 0.0248840263858\n",
      "GiftCntCard36 : 0.0194517444668\n",
      "DemHomeOwner : 0.0126378101702\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# grab feature importances from the model and feature name from the original X\n",
    "importances = model.feature_importances_\n",
    "feature_names = X.columns\n",
    "\n",
    "# sort them out in descending order\n",
    "indices = np.argsort(importances)\n",
    "indices = np.flip(indices, axis=0)\n",
    "\n",
    "# limit to 20 features, you can leave this out to print out everything\n",
    "indices = indices[:20]\n",
    "\n",
    "for i in indices:\n",
    "    print(feature_names[i], ':', importances[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In descending order, the top 3 important variables for this model are `DemMedHomeValue`, `DemIncome` and `DemAge`. Feature importance is really important to not only understand the model, but also to learn more about the data and present conclusions to stakeholders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Visualising decision tree structure\n",
    "\n",
    "Other than feature importance, we could also gain insights of our decision tree by visualising it. To do this, use the `export_graphviz` function and `pydot` module. Open the `.png` file to view the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pydot\n",
    "from io import StringIO\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "# visualize\n",
    "dotfile = StringIO()\n",
    "export_graphviz(model, out_file=dotfile, feature_names=X.columns)\n",
    "graph = pydot.graph_from_dot_data(dotfile.getvalue())\n",
    "graph[0].write_png(\"week3_dt_viz.png\") # saved in the following file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process will take a long time and your picture will be very large and incomprehensible. This shows that the model is very complex and deep, which is a typical characteristic of an overfitting model.\n",
    "\n",
    "Let's limit the complexity of the model by setting the `max_depth` that the model can go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.583935577122\n",
      "Test accuracy: 0.559776997729\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.55      0.72      0.62      2422\n",
      "          1       0.59      0.40      0.48      2421\n",
      "\n",
      "avg / total       0.57      0.56      0.55      4843\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#retrain with a small max_depth limit\n",
    "\n",
    "model = DecisionTreeClassifier(max_depth=3)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Train accuracy:\", model.score(X_train, y_train))\n",
    "print(\"Test accuracy:\", model.score(X_test, y_test))\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could see that the simpler model (small `max_depth`) actually performs much better on the test data. `max_depth` on a decision tree is what we call a hyperparameter (or just parameter) and they are responsible for the structure of a model. Different combinations of parameters will produce different models with different performance too.\n",
    "\n",
    "Let's do a feature importance analysis and visualization on this new decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GiftAvgLast : 0.49832551516\n",
      "DemMedHomeValue : 0.179905317927\n",
      "GiftAvgCard36 : 0.125956067503\n",
      "GiftTimeLast : 0.103066735755\n",
      "GiftCnt36 : 0.0490047340628\n",
      "DemAge : 0.0437416295918\n",
      "DemCluster_11 : 0.0\n",
      "StatusCat96NK_L : 0.0\n",
      "StatusCat96NK_N : 0.0\n",
      "StatusCat96NK_S : 0.0\n",
      "DemCluster_0 : 0.0\n",
      "DemCluster_1 : 0.0\n",
      "DemCluster_10 : 0.0\n",
      "DemCluster_13 : 0.0\n",
      "DemCluster_12 : 0.0\n",
      "StatusCat96NK_E : 0.0\n",
      "DemCluster_14 : 0.0\n",
      "DemCluster_15 : 0.0\n",
      "DemCluster_16 : 0.0\n",
      "DemCluster_17 : 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grab feature importances from the model and feature name from the original X\n",
    "importances = model.feature_importances_\n",
    "feature_names = X.columns\n",
    "\n",
    "# sort them out in descending order\n",
    "indices = np.argsort(importances)\n",
    "indices = np.flip(indices, axis=0)\n",
    "\n",
    "# limit to 20 features, you can leave this out to print out everything\n",
    "indices = indices[:20]\n",
    "\n",
    "for i in indices:\n",
    "    print(feature_names[i], ':', importances[i])\n",
    "\n",
    "# visualize\n",
    "dotfile = StringIO()\n",
    "export_graphviz(model, out_file=dotfile, feature_names=X.columns)\n",
    "graph = pydot.graph_from_dot_data(dotfile.getvalue())\n",
    "graph[0].write_png(\"week3_dt_viz.png\") # saved in the following file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Simple decision tree structure](http://dataminingtuts.s3.amazonaws.com/week3_dt_viz.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks better now. You can see that the decision tree start splitting with `GiftAvgLast`, with `GiftAvgCard36` and `GiftTimeLast` as the competing splits. We have around 8 leaf nodes here, and you can see the number of samples and value splits in each node (e.g. for the left most leaf, we have 757 customers here, with distribution of 262 non-donors to 495 donors).\n",
    "\n",
    "Now, the question is, how do we find the optimal combination of parameters for a model on a given dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Finding optimal hyperparameters with GridSearchCV\n",
    "\n",
    "A common method to find the optimal set of parameters for a model is to run a exhaustive search over all possible values of each parameter. To choose the best model among all candidates, k-fold cross validation is typically used.\n",
    "\n",
    "In k-fold cross-validation, the training dataset is randomly partitioned into `k` equal size partitions. Of the k subsamples, a single subsample is retained as the validation data for testing the model, and the remaining k-1 subsamples are used as training data. The cross-validation process is then repeated k times (the folds), with each of the k subsamples used exactly once as the validation data. The k results from the folds can then be averaged (or otherwise combined) to produce a single estimation, which we will use to choose the best model. (courtesy of openml.org)\n",
    "\n",
    "In `sklearn`, the exhaustive search + k-fold validation is implemented in `GridSearchCV`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform a GridSearchCV, we first have to determine the hyperparameters and possible values of parameters that we want to use. Each class of predictive models will have different kind of parameters (e.g. decision tree will be different to a regression). For this tutorial, we will search on 3 hyperparameters:\n",
    "1. Criterion: The function to measure the quality of a split. There are two criterias we will use, “gini” for the Gini impurity and “entropy” for the information gain.\n",
    "2. Max depth: The maximum depth of the tree. Let's start with range of 2-10.\n",
    "3. Min samples leaf: The minimum number of samples required to be at a leaf node, allowing us to limit the minimum size of a leaf node. Let's start with range of 20-50 with step of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.582696675614\n",
      "Test accuracy: 0.559570514144\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.55      0.72      0.62      2422\n",
      "          1       0.59      0.40      0.48      2421\n",
      "\n",
      "avg / total       0.57      0.56      0.55      4843\n",
      "\n",
      "{'criterion': 'entropy', 'max_depth': 3, 'min_samples_leaf': 50}\n"
     ]
    }
   ],
   "source": [
    "# grid search CV\n",
    "params = {'criterion': ['gini', 'entropy'],\n",
    "          'max_depth': range(3, 10),\n",
    "          'min_samples_leaf': range(20, 60, 10)}\n",
    "\n",
    "cv = GridSearchCV(param_grid=params, estimator=DecisionTreeClassifier(), cv=10)\n",
    "cv.fit(X_train, y_train)\n",
    "\n",
    "print(\"Train accuracy:\", cv.score(X_train, y_train))\n",
    "print(\"Test accuracy:\", cv.score(X_test, y_test))\n",
    "\n",
    "# test the best model\n",
    "y_pred = cv.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# print parameters of the best model\n",
    "print(cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the test `accuracy` of our model is around the same with the previous best. Let's see if we can further optimise on the found best parameters.\n",
    "\n",
    "Let's do another grid search, now being more specific around the best params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.574850299401\n",
      "Test accuracy: 0.567210406773\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.56      0.62      0.59      2422\n",
      "          1       0.58      0.51      0.54      2421\n",
      "\n",
      "avg / total       0.57      0.57      0.57      4843\n",
      "\n",
      "{'criterion': 'gini', 'max_depth': 2, 'min_samples_leaf': 40}\n"
     ]
    }
   ],
   "source": [
    "# grid search CV #2\n",
    "params = {'criterion': ['gini'],\n",
    "          'max_depth': range(2, 5),\n",
    "          'min_samples_leaf': range(40, 61, 5)}\n",
    "\n",
    "cv = GridSearchCV(param_grid=params, estimator=DecisionTreeClassifier(), cv=10)\n",
    "cv.fit(X_train, y_train)\n",
    "\n",
    "print(\"Train accuracy:\", cv.score(X_train, y_train))\n",
    "print(\"Test accuracy:\", cv.score(X_test, y_test))\n",
    "\n",
    "# test the best model\n",
    "y_pred = cv.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# print parameters of the best model\n",
    "print(cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `accuracy` is improved over the previous best, which means our parameter tuning is successful. The model ended up being really simple, being only 2 levels deep. Try to experiment with other kinds of parameters and see whether you could do better than this. You can find the list of parameters available in a decision tree in `sklearn`'s `DecisionTreeClassifier` documentation website.\n",
    "\n",
    "## Tips\n",
    "\n",
    "1. Always perform feature importance and visualization to understand your decision tree. The best model can be found in `cv.best_estimator_`.\n",
    "2. We will use feature importance a lot across this decision tree modelling process. Rather than writing the script multiple times (which is tedious and error-prone), we can just wrap them in functions in `dm_tools.py` and import it from there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# inside `dm_tools.py'\n",
    "import numpy as np\n",
    "import pydot\n",
    "from io import StringIO\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "def analyse_feature_importance(dm_model, feature_names, n_to_display=20):\n",
    "    # grab feature importances from the model\n",
    "    importances = dm_model.feature_importances_\n",
    "    \n",
    "    # sort them out in descending order\n",
    "    indices = np.argsort(importances)\n",
    "    indices = np.flip(indices, axis=0)\n",
    "\n",
    "    # limit to 20 features, you can leave this out to print out everything\n",
    "    indices = indices[:n_to_display]\n",
    "\n",
    "    for i in indices:\n",
    "        print(feature_names[i], ':', importances[i])\n",
    "\n",
    "def visualize_decision_tree(dm_model, feature_names, save_name):\n",
    "    dotfile = StringIO()\n",
    "    export_graphviz(dm_model, out_file=dotfile, feature_names=feature_names)\n",
    "    graph = pydot.graph_from_dot_data(dotfile.getvalue())\n",
    "    graph[0].write_png(save_name) # saved in the following file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GiftAvgLast : 0.685126372799\n",
      "GiftAvgCard36 : 0.173171593738\n",
      "GiftTimeLast : 0.141702033462\n",
      "DemGender_U : 0.0\n",
      "DemCluster_11 : 0.0\n",
      "StatusCat96NK_F : 0.0\n",
      "StatusCat96NK_L : 0.0\n",
      "StatusCat96NK_N : 0.0\n",
      "StatusCat96NK_S : 0.0\n",
      "DemCluster_0 : 0.0\n",
      "DemCluster_1 : 0.0\n",
      "DemCluster_10 : 0.0\n",
      "DemCluster_13 : 0.0\n",
      "DemCluster_12 : 0.0\n",
      "StatusCat96NK_A : 0.0\n",
      "DemCluster_14 : 0.0\n",
      "DemCluster_15 : 0.0\n",
      "DemCluster_16 : 0.0\n",
      "DemCluster_17 : 0.0\n",
      "DemCluster_18 : 0.0\n"
     ]
    }
   ],
   "source": [
    "# do the feature importance and visualization analysis on GridSearchCV's best model\n",
    "from dm_tools import analyse_feature_importance, visualize_decision_tree\n",
    "\n",
    "analyse_feature_importance(cv.best_estimator_, X.columns, 20)\n",
    "visualize_decision_tree(cv.best_estimator_, X.columns, \"dm_best_cv.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![GridSearchCV decision tree](http://dataminingtuts.s3.amazonaws.com/dm_best_cv.png)\n",
    "\n",
    "## End notes and next week\n",
    "\n",
    "This week, we learned how to perform data partitioning, build decision trees and test them, and finding the optimal parameter sets using GridSearchCV.\n",
    "\n",
    "Next week, we will focus on performing predictive classification modelling using logistic regression."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
