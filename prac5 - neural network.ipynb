{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5: Neural Network\n",
    "\n",
    "### What's on this week\n",
    "1. [Resuming from week 4](#resume)\n",
    "2. [Building your first neural network model](#build)\n",
    "3. [Understanding your neural network model](#viz)\n",
    "4. [Finding optimal hyperparameters with GridSearchCV](#gridsearch)\n",
    "5. [Feature selection](#fselect)\n",
    "6. [Comparing each model](#comparison)\n",
    "\n",
    "---\n",
    "\n",
    "### Important Changelog:\n",
    "* (25/07/2017) Made tutorial notes public.\n",
    "* (28/08/2017) Finally bringing myself to finish the explanation. Sorry guys :)\n",
    "\n",
    "The practical note for this week introduces you to neural network mining in Python, particularly using multilayer perceptron classifier. Neural networks are a class of predictive models that mimic the structure of human brain. It consists of layers of neurons, each consuming outputs from the previous layers as inputs. Neural network is the most complex model out of everything that we have used so far.\n",
    "\n",
    "**This tutorial notes is in experimental version. Please give us feedbacks and suggestions on how to make it better. Ask your tutor for any question and clarification.**\n",
    "\n",
    "## 1. Resuming from week 4<a name=\"resume\"></a>\n",
    "Last week, we learned how to perform data mining with decision trees in Python. For this week, we will again reuse the code for data preprocessing. Just as regression models, neural networks are sensitive to skewed data, thus we also need to perform standarization on the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from dm_tools import data_prep\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# preprocessing step\n",
    "df = data_prep()\n",
    "\n",
    "# train test split\n",
    "y = df['TargetB']\n",
    "X = df.drop(['TargetB'], axis=1)\n",
    "X_mat = X.as_matrix()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_mat, y, test_size=0.5, random_state=42, stratify=y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train, y_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2. Building your first neural network model\n",
    "\n",
    "Start by importing your neural network from the library. As mentioned in the previous weeks, our aim is to classify people on this dataset into donors and non-donors. In `sklearn`, one neural network designed for that purpose is `MLPClassifier`, short of multi-layer perceptron classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train our first MLPClassifier. Initiate the model without any additional parameter and see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.933718769358\n",
      "Test accuracy: 0.53128226306\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.53      0.53      2422\n",
      "          1       0.53      0.54      0.53      2421\n",
      "\n",
      "avg / total       0.53      0.53      0.53      4843\n",
      "\n",
      "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
      "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
      "       verbose=False, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lieh\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "model = MLPClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Train accuracy:\", model.score(X_train, y_train))\n",
    "print(\"Test accuracy:\", model.score(X_test, y_test))\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first neural network did pretty okay, with 0.535 accuracy score on the test data. You should notice that the training accuracy is much higher than the test accuracy. This is again an indication of overfitting, which we will fix in the next steps.\n",
    "\n",
    "In addition, we got a \"convergence is not reached\" warning message from the neural network. Just as regression, neural networks compute values/predict using activation function with input weights in each neuron. These weights are learned in an iterative  process called \"backpropagation\" and in `MLPClassifier` the number of iteration is limited by `max_iter` parameter (default 200). The warning message basically tell us that the `max_iter` value is not enough to finish the backpropagation process, which we have to increase the number of max iterations.\n",
    "\n",
    "Try to increase the `max_iter` to 500 from the original 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.973363617592\n",
      "Test accuracy: 0.529836877968\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.53      0.53      2422\n",
      "          1       0.53      0.53      0.53      2421\n",
      "\n",
      "avg / total       0.53      0.53      0.53      4843\n",
      "\n",
      "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=500, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
      "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
      "       verbose=False, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "model = MLPClassifier(max_iter=500)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Train accuracy:\", model.score(X_train, y_train))\n",
    "print(\"Test accuracy:\", model.score(X_test, y_test))\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 500 max iterations, we managed to reach convergence. Let's keep this value. You can set up the max iter value higher to guarantee convergence everytime, at cost of slower training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Finding optimal hyperparameter with GridSearchCV\n",
    "\n",
    "Once we have trained our first neural network, we should tune it to increase its performance using GridSearchCV. Neural network is one of the hardest models to tune due to its relatively high number of parameters and slow training process. For this tutorial, let's focus on tuning two parameters:\n",
    "1. `hidden_layer_sizes`: It has values of tuples, and within each tuple, element i-th represent the number of neurons contained in each hidden layer.\n",
    "2. `alpha`: L2 regularization parameter used in each neuron's activation function.\n",
    "\n",
    "Firstly, let's start by tuning the hidden layer sizes. There is no official guideline on how many neurons we should have in each layer, but for most cases it is sufficient to have a single hidden layer with neurons count between `number of features` (input variables) and `output neurons` (in this case, it is 2).\n",
    "\n",
    "**Fun Fact:** You might have heard of deep learning, which is a process of building very complex neural networks (these models often have hundreds of layers and thousands of neurons, hence **deep**). They are typically used for high level tasks, like image recognition, Siri-like voice assistant, machine translation and beating Go world champion. You might contribute in building the next Skynet here... :)\n",
    "\n",
    "Let's see how many input features we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4843, 85)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 85 features, we will start tuning with one hidden layer of 5 to 85 neurons, increment of 25. (This is going to be slow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.636795374768\n",
      "Test accuracy: 0.549039851332\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.55      0.56      0.56      2422\n",
      "          1       0.55      0.53      0.54      2421\n",
      "\n",
      "avg / total       0.55      0.55      0.55      4843\n",
      "\n",
      "{'hidden_layer_sizes': (5,)}\n"
     ]
    }
   ],
   "source": [
    "params = {'hidden_layer_sizes': [(x,) for x in range(5, 86, 25)]}\n",
    "\n",
    "cv = GridSearchCV(param_grid=params, estimator=MLPClassifier(max_iter=500), cv=10, n_jobs=-1)\n",
    "cv.fit(X_train, y_train)\n",
    "\n",
    "print(\"Train accuracy:\", cv.score(X_train, y_train))\n",
    "print(\"Test accuracy:\", cv.score(X_test, y_test))\n",
    "\n",
    "y_pred = cv.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our GridSearchCV returns 5 neurons as the best setting. For this dataset, it seems like more complex models (more neurons you have in your hidden layer) tend to overfit. We might want to tune lower neuron count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.622341523849\n",
      "Test accuracy: 0.556060293207\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.56      0.54      0.55      2422\n",
      "          1       0.55      0.58      0.57      2421\n",
      "\n",
      "avg / total       0.56      0.56      0.56      4843\n",
      "\n",
      "{'hidden_layer_sizes': (3,)}\n"
     ]
    }
   ],
   "source": [
    "params = {'hidden_layer_sizes': [(3,), (5,), (7,), (9,)]}\n",
    "\n",
    "cv = GridSearchCV(param_grid=params, estimator=MLPClassifier(max_iter=500), cv=10, n_jobs=-1)\n",
    "cv.fit(X_train, y_train)\n",
    "\n",
    "print(\"Train accuracy:\", cv.score(X_train, y_train))\n",
    "print(\"Test accuracy:\", cv.score(X_test, y_test))\n",
    "\n",
    "y_pred = cv.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like we have our answer for the neuron count. Once we have it, we will try to tune `alpha`. The default value is `0.0001`, thus let's try to tune around it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.616147016312\n",
      "Test accuracy: 0.557712161883\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.55      0.61      0.58      2422\n",
      "          1       0.56      0.50      0.53      2421\n",
      "\n",
      "avg / total       0.56      0.56      0.56      4843\n",
      "\n",
      "{'alpha': 0.0001, 'hidden_layer_sizes': (3,)}\n"
     ]
    }
   ],
   "source": [
    "params = {'hidden_layer_sizes': [(3,), (5,), (7,), (9,)], 'alpha': [0.01,0.001, 0.0001, 0.00001]}\n",
    "\n",
    "cv = GridSearchCV(param_grid=params, estimator=MLPClassifier(max_iter=500), cv=10, n_jobs=-1)\n",
    "cv.fit(X_train, y_train)\n",
    "\n",
    "print(\"Train accuracy:\", cv.score(X_train, y_train))\n",
    "print(\"Test accuracy:\", cv.score(X_test, y_test))\n",
    "\n",
    "y_pred = cv.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dimensionality reduction\n",
    "\n",
    "Now, let's try to reduce the size of our feature set and see whether it improves the performance of the model. We will use the same techniques as covered last week.\n",
    "\n",
    "### 5.1. Recursive Feature Elimination\n",
    "\n",
    "Firstly, reduce the feature set size using RFE. We will need a base elimination model and RFE requires type of model that assigns weight/feature importance to each feature (like regression/decision tree). Unfortunately, neural networks provide neither, thus we will try to use LogisticRegression as the base elimination model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "rfe = RFECV(estimator = LogisticRegression(), cv=10)\n",
    "rfe.fit(X_train, y_train)\n",
    "\n",
    "print(rfe.n_features_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RFE has selected 19 features as the best set of features. Next, tune an `MLPClassifier` with the transformed data set as training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.604377451993\n",
      "Test accuracy: 0.562048317159\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.56      0.58      0.57      2422\n",
      "          1       0.56      0.54      0.55      2421\n",
      "\n",
      "avg / total       0.56      0.56      0.56      4843\n",
      "\n",
      "{'alpha': 0.0001, 'hidden_layer_sizes': (9,)}\n"
     ]
    }
   ],
   "source": [
    "X_train_rfe = rfe.transform(X_train)\n",
    "X_test_rfe = rfe.transform(X_test)\n",
    "\n",
    "# step = int((X_train_rfe.shape[1] + 5)/5);\n",
    "params = {'hidden_layer_sizes': [(3,), (5,), (7,), (9,)], 'alpha': [0.01,0.001, 0.0001, 0.00001]}\n",
    "\n",
    "cv = GridSearchCV(param_grid=params, estimator=MLPClassifier(max_iter=1000), cv=10, n_jobs=-1)\n",
    "cv.fit(X_train_rfe, y_train)\n",
    "\n",
    "print(\"Train accuracy:\", cv.score(X_train_rfe, y_train))\n",
    "print(\"Test accuracy:\", cv.score(X_test_rfe, y_test))\n",
    "\n",
    "y_pred = cv.predict(X_test_rfe)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RFE selected feature set showed major improvements over the original data set. We managed to bring train/test accuracy closer and produce a model that generalise better.\n",
    "\n",
    "As mentioned before, we could also use decision trees in RFE. Let's try to do it with `DecisionTreeClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "rfe = RFECV(estimator = DecisionTreeClassifier(), cv=10)\n",
    "rfe.fit(X_train, y_train)\n",
    "\n",
    "print(rfe.n_features_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.586619863721\n",
      "Test accuracy: 0.569894693372\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.57      0.56      0.57      2422\n",
      "          1       0.57      0.58      0.57      2421\n",
      "\n",
      "avg / total       0.57      0.57      0.57      4843\n",
      "\n",
      "{'alpha': 1e-05, 'hidden_layer_sizes': (7,)}\n"
     ]
    }
   ],
   "source": [
    "X_train_rfe = rfe.transform(X_train)\n",
    "X_test_rfe = rfe.transform(X_test)\n",
    "\n",
    "# step = int((X_train_rfe.shape[1] + 5)/5);\n",
    "params = {'hidden_layer_sizes': [(3,), (5,), (7,), (9,)], 'alpha': [0.01,0.001, 0.0001, 0.00001]}\n",
    "\n",
    "cv = GridSearchCV(param_grid=params, estimator=MLPClassifier(max_iter=1000), cv=10, n_jobs=-1)\n",
    "cv.fit(X_train_rfe, y_train)\n",
    "\n",
    "print(\"Train accuracy:\", cv.score(X_train_rfe, y_train))\n",
    "print(\"Test accuracy:\", cv.score(X_test_rfe, y_test))\n",
    "\n",
    "y_pred = cv.predict(X_test_rfe)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This decision tree classifier RFE selects 29 features, which the tuned neural network produces slightly different performance than the LogisticRegression RFE. In real life practice, both approaches are valid and you should try both when faced with a new data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Principle Component Analysis\n",
    "\n",
    "The second feature reduction technique that we will try is principle component analysis, or PCA. As mentioned last week, PCA is a technique that finds underlying variables (known as principal components) that best differentiate your data points. The idea of PCA is to reduce the number of features while still retaining the variance/pattern in the feature set. As last week, we will set our variance threshold at 95%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N components with > 95% variance = 66\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(X_train)\n",
    "\n",
    "sum_var = 0\n",
    "for idx, val in enumerate(pca.explained_variance_ratio_):\n",
    "    sum_var += val\n",
    "    if (sum_var >= 0.95):\n",
    "        print(\"N components with > 95% variance =\", idx+1)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "66 components with cumulative > 95% variance are selected. Train and test on this transformed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.601693165393\n",
      "Test accuracy: 0.549452818501\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.55      0.52      0.54      2422\n",
      "          1       0.55      0.58      0.56      2421\n",
      "\n",
      "avg / total       0.55      0.55      0.55      4843\n",
      "\n",
      "{'alpha': 0.01, 'hidden_layer_sizes': (3,)}\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=66)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "params = {'hidden_layer_sizes': [(3,), (5,), (7,), (9,)], 'alpha': [0.01,0.001, 0.0001, 0.00001]}\n",
    "\n",
    "cv = GridSearchCV(param_grid=params, estimator=MLPClassifier(max_iter=1000), cv=10, n_jobs=-1)\n",
    "cv.fit(X_train_pca, y_train)\n",
    "\n",
    "print(\"Train accuracy:\", cv.score(X_train_pca, y_train))\n",
    "print(\"Test accuracy:\", cv.score(X_test_pca, y_test))\n",
    "\n",
    "y_pred = cv.predict(X_test_pca)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result shows an improved performance over the original feature set. We also managed to reduce the feature set size to only 66, which shorten the training process. However, compared to the RFE selected feature set, PCA still produces slightly worse performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Selecting using decision tree\n",
    "\n",
    "Lastly, we will use decision tree and feature importance produces from the model to perform feature selection. To start, we need to tune a decision tree with GridSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise',\n",
       "       estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'criterion': ['gini', 'entropy'], 'max_depth': range(3, 10), 'min_samples_leaf': range(20, 200, 20)},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "params = {'criterion': ['gini', 'entropy'],\n",
    "          'max_depth': range(3, 10),\n",
    "          'min_samples_leaf': range(20, 200, 20)}\n",
    "\n",
    "cv = GridSearchCV(param_grid=params, estimator=DecisionTreeClassifier(), cv=10)\n",
    "cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GiftAvgLast : 0.424147866229\n",
      "DemMedHomeValue : 0.165964132384\n",
      "GiftTimeLast : 0.148495742668\n",
      "GiftAvgCard36 : 0.0996893965541\n",
      "DemAge : 0.0654544865005\n",
      "PromCntCard36 : 0.0494236862349\n",
      "GiftCntAll : 0.0468246894295\n",
      "DemGender_U : 0.0\n",
      "DemCluster_11 : 0.0\n",
      "StatusCat96NK_N : 0.0\n",
      "StatusCat96NK_S : 0.0\n",
      "DemCluster_0 : 0.0\n",
      "DemCluster_1 : 0.0\n",
      "DemCluster_10 : 0.0\n",
      "DemCluster_13 : 0.0\n",
      "DemCluster_12 : 0.0\n",
      "StatusCat96NK_F : 0.0\n",
      "DemCluster_14 : 0.0\n",
      "DemCluster_15 : 0.0\n",
      "DemCluster_16 : 0.0\n"
     ]
    }
   ],
   "source": [
    "from dm_tools import analyse_feature_importance\n",
    "\n",
    "analyse_feature_importance(cv.best_estimator_, X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4843, 7)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "selectmodel = SelectFromModel(cv.best_estimator_, prefit=True)\n",
    "X_train_sel_model = selectmodel.transform(X_train)\n",
    "X_test_sel_model = selectmodel.transform(X_test)\n",
    "\n",
    "print(X_train_sel_model.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our `analyse_feature_importance` function shows around 7 important features according to this decision tree model. With this result, `SelectFromModel` transforms the original dataset into only 7 columns. Proceed to tune a MLPClassifier with this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.578567003923\n",
      "Test accuracy: 0.571340078464\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.57      0.60      0.58      2422\n",
      "          1       0.58      0.54      0.56      2421\n",
      "\n",
      "avg / total       0.57      0.57      0.57      4843\n",
      "\n",
      "{'alpha': 0.0001, 'hidden_layer_sizes': (9,)}\n"
     ]
    }
   ],
   "source": [
    "params = {'hidden_layer_sizes': [(3,), (5,), (7,), (9,)], 'alpha': [0.01,0.001, 0.0001, 0.00001]}\n",
    "\n",
    "cv = GridSearchCV(param_grid=params, estimator=MLPClassifier(max_iter=1000), cv=10, n_jobs=-1)\n",
    "cv.fit(X_train_sel_model, y_train)\n",
    "\n",
    "print(\"Train accuracy:\", cv.score(X_train_sel_model, y_train))\n",
    "print(\"Test accuracy:\", cv.score(X_test_sel_model, y_test))\n",
    "\n",
    "y_pred = cv.predict(X_test_sel_model)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test accuracy result shows improvement over the original feature set and RFE selected feature set as well. This method yields the smallest feature set yet (only 7 rather than 85 features), and again the performance is the best. We will keep this model as the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparing Models\n",
    "\n",
    "After 5 weeks, we have learned how to perform data preprocessing, model tuning and various dimensionality reduction techniques. While we have been pretty straight forward with our decisions and process, in real life projects, you need to test many different techniques/approaches before getting to the solution. You need to constantly expand your tool box and knowledge to be a great data miner/data scientist/machine learning engineer.\n",
    "\n",
    "Now, let's imagine that you have built multiple models, where each uses different data preprocessing/dimensionality reduction techniques. How do you choose the best performing model? One way to do this is by comparing statistics produced by each model. I will show you how.\n",
    "\n",
    "Firstly, let's train and tune three models of `DecisionTreeClassifier`, `LogisticRegression` and `MLPClassifier` with GridSearchCV. We will use the original feature set with no dimensionality reduction for this demonstration, but the process should not be too much different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=2,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=40,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "LogisticRegression(C=0.0001, class_weight=None, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n",
      "MLPClassifier(activation='relu', alpha=0.001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(7,), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=500, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
      "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
      "       verbose=False, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "# grid search CV for decision tree\n",
    "params_dt = {'criterion': ['gini'],\n",
    "          'max_depth': range(2, 5),\n",
    "          'min_samples_leaf': range(40, 61, 5)}\n",
    "\n",
    "cv = GridSearchCV(param_grid=params_dt, estimator=DecisionTreeClassifier(), cv=10)\n",
    "cv.fit(X_train, y_train)\n",
    "\n",
    "dt_model = cv.best_estimator_\n",
    "print(dt_model)\n",
    "\n",
    "# grid search CV for logistic regression\n",
    "params_log_reg = {'C': [pow(10, x) for x in range(-6, 4)]}\n",
    "\n",
    "cv = GridSearchCV(param_grid=params_log_reg, estimator=LogisticRegression(), cv=10, n_jobs=-1)\n",
    "cv.fit(X_train, y_train)\n",
    "\n",
    "log_reg_model = cv.best_estimator_\n",
    "print(log_reg_model)\n",
    "\n",
    "# grid search CV for NN\n",
    "params_nn = {'hidden_layer_sizes': [(3,), (5,), (7,), (9,)], 'alpha': [0.01,0.001, 0.0001, 0.00001]}\n",
    "\n",
    "cv = GridSearchCV(param_grid=params_nn, estimator=MLPClassifier(max_iter=500), cv=10, n_jobs=-1)\n",
    "cv.fit(X_train, y_train)\n",
    "\n",
    "nn_model = cv.best_estimator_\n",
    "print(nn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Test Accuracy\n",
    "\n",
    "Once you have them trained, there are a number of statistics that we could use for comparing models. First, accuracy of the models on test data, just like what we have used so far.\n",
    "\n",
    "**Note**: Accuracy is a great statistics when the ratio of target classes are relatively equal, like what we have in this dataset (50% donors vs 50% non-donors). In cases where the targets are not equal (e.g. in cancer detection task where most people in the dataset will not have cancer), metrics like precision/recall/F1 from `classification_report` or Cohen's kappa are preferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score on test for DT: 0.567210406773\n",
      "Accuracy score on test for logistic regression: 0.569275242618\n",
      "Accuracy score on test for NN: 0.547387982655\n"
     ]
    }
   ],
   "source": [
    "y_pred_dt = dt_model.predict(X_test)\n",
    "y_pred_log_reg = log_reg_model.predict(X_test)\n",
    "y_pred_nn = nn_model.predict(X_test)\n",
    "\n",
    "print(\"Accuracy score on test for DT:\", accuracy_score(y_test, y_pred_dt))\n",
    "print(\"Accuracy score on test for logistic regression:\", accuracy_score(y_test, y_pred_log_reg))\n",
    "print(\"Accuracy score on test for NN:\", accuracy_score(y_test, y_pred_nn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On test accuracy score, logistic regression performs the best, followed by decision tree and neural network.\n",
    "\n",
    "### 6.2. ROC AUC\n",
    "Another metric commonly used to compare models is receiver operating characteristic (ROC) and area under curve (AUC). ROC refers to the ability of binary classifier (like what we have here) to classify with varied discrimination threshold.\n",
    "\n",
    "Most predictive classification models produce probability of target values on a set of inputs. `LogisticRegression` and `MLPClassifier` produces real value probabilities, while `DecisionTree` has the ratio of majority classes in each leaf node. Most of the time, discrimination threshold is cap at 0.5, which means any probability prediction above 0.5 is considered as positive (and the rest negative). For more clarity, see this code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability produced by decision tree for each class vs actual prediction on TargetB (0 = non-donor, 1 = donor). You should be able to see the default threshold of 0.5.\n",
      "(Probs on zero)\t(probs on one)\t(prediction made)\n",
      "0.336956521739 \t 0.663043478261 \t 1\n",
      "0.569136745607 \t 0.430863254393 \t 0\n",
      "0.336956521739 \t 0.663043478261 \t 1\n",
      "0.461200585652 \t 0.538799414348 \t 1\n",
      "0.461200585652 \t 0.538799414348 \t 1\n",
      "0.569136745607 \t 0.430863254393 \t 0\n",
      "0.569136745607 \t 0.430863254393 \t 0\n",
      "0.569136745607 \t 0.430863254393 \t 0\n",
      "0.569136745607 \t 0.430863254393 \t 0\n",
      "0.461200585652 \t 0.538799414348 \t 1\n"
     ]
    }
   ],
   "source": [
    "# typical prediction\n",
    "y_pred = dt_model.predict(X_test)\n",
    "\n",
    "# probability prediction from decision tree\n",
    "y_pred_proba_dt = dt_model.predict_proba(X_test)\n",
    "\n",
    "print(\"Probability produced by decision tree for each class vs actual prediction on TargetB (0 = non-donor, 1 = donor). You should be able to see the default threshold of 0.5.\")\n",
    "print(\"(Probs on zero)\\t(probs on one)\\t(prediction made)\")\n",
    "# print top 10\n",
    "for i in range(10):\n",
    "    print(y_pred_proba_dt[i][0], '\\t', y_pred_proba_dt[i][1], '\\t', y_pred[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this concept in mind, ROC AUC score aims to find the best model under varied threshold. To compute our ROC AUC score, use the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC index on test for DT: 0.577610714942\n",
      "ROC index on test for logistic regression: 0.595601520006\n",
      "ROC index on test for NN: 0.573426128587\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "y_pred_proba_dt = dt_model.predict_proba(X_test)\n",
    "y_pred_proba_log_reg = log_reg_model.predict_proba(X_test)\n",
    "y_pred_proba_nn = nn_model.predict_proba(X_test)\n",
    "\n",
    "roc_index_dt = roc_auc_score(y_test, y_pred_proba_dt[:, 1])\n",
    "roc_index_log_reg = roc_auc_score(y_test, y_pred_proba_log_reg[:, 1])\n",
    "roc_index_nn = roc_auc_score(y_test, y_pred_proba_nn[:, 1])\n",
    "\n",
    "print(\"ROC index on test for DT:\", roc_index_dt)\n",
    "print(\"ROC index on test for logistic regression:\", roc_index_log_reg)\n",
    "print(\"ROC index on test for NN:\", roc_index_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`LogisticRegression` produces the best ROC score. This means on varied discrimination threshold, this LogReg model performs better compared to the other two models.\n",
    "\n",
    "ROC score only tells a side of the story, however. Typically, instead of ROC score, we plot a curve to show the performance of the model on different threshold values. The curve should look something like this, and the closer the curve is to top left corner, the better the model is.\n",
    "\n",
    "![ROC Curve](http://gim.unmc.edu/dxtests/roccomp.jpg)\n",
    "\n",
    "Let's plot ROC curve for our models. Firstly, we need to find the false positive rate, true positive rate and thresholds used for each models. We can get it from the `roc_curve` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr_dt, tpr_dt, thresholds_dt = roc_curve(y_test, y_pred_proba_dt[:,1])\n",
    "fpr_log_reg, tpr_log_reg, thresholds_log_reg = roc_curve(y_test, y_pred_proba_log_reg[:,1])\n",
    "fpr_nn, tpr_nn, thresholds_nn = roc_curve(y_test, y_pred_proba_nn[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have these scores, plot them using `matplotlib`'s pyplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYDBky/8D3UVdxsY/4SUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xb9b04a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(fpr_dt, tpr_dt, label='ROC Curve for DT {:.3f}'.format(roc_index_dt), color='red', lw=0.5)\n",
    "plt.plot(fpr_log_reg, tpr_log_reg, label='ROC Curve for Log reg {:.3f}'.format(roc_index_log_reg), color='green', lw=0.5)\n",
    "plt.plot(fpr_nn, tpr_nn, label='ROC Curve for NN {:.3f}'.format(roc_index_nn), color='darkorange', lw=0.5)\n",
    "\n",
    "# plt.plot(fpr[2], tpr[2], color='darkorange',\n",
    "#          lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[2])\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=0.5, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Here, you can see the curve for different models. `LogisticRegression` again has the largest curve area compared to the other two models. Thus, all three statistics that we used collectively agreed on `LogisticRegression` being the best performing model overall.\n",
    "\n",
    "While statistics are vital, in a real project, performance is not always the priority. Some of the other aspects used to consider a best model are:\n",
    "1. Interpretability: how well can humans use the model to make decisions. Decision trees (and regressions to some extent) excel at this, while neural network not so much.\n",
    "2. Speed: how well can the model train and predict on large amount of data. Again decision trees and regressions are relatively fast, while neural networks take a while to train.\n",
    "3. Adaptability: in some cases, you want your model to slowly adapt to the data trend. Neural networks are great for this as they can be trained using \"online training\", while decision trees are not so great.\n",
    "\n",
    "## End Notes and Next Week\n",
    "\n",
    "This week, we learned how to build, tune and explore the structure of neural network models. We also explored dimensionality reduction techniques to reduce the size of the feature set and improve performance of our neural network model. In addition, we tried numerous statistics to compare end-to-end performance of all models we have built so far.\n",
    "\n",
    "Next week, we will have a drop-in help session where you focus on the assignment and ask your tutor questions regarding it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
